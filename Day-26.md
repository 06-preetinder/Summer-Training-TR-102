📅 Day 26 – Summer Training at AppTechies

Date: July 30, 2025
Training Domain: Artificial Intelligence
Company: AppTechies

🧠 What I Did Today

Today I focused on diving deeper into the internal mechanics of neural networks.

Topics Covered:

Forward Propagation:
Learned how data flows through layers, activating neurons step-by-step.

Backpropagation:
Understood how the model learns by propagating the error backwards and updating weights using gradients.

Gradient Descent:
Explored how this optimization algorithm minimizes the loss function to improve model performance.

Vanishing Gradient Problem:
Studied the challenge of gradients becoming too small during backpropagation in deep networks, making training difficult.


🔍 Insights & Takeaways

The importance of weight initialization and activation functions in mitigating vanishing gradients.

Realized why ReLU is often used to tackle vanishing gradient issues.

Understood how backpropagation is the backbone of learning in deep neural networks.


🚀 Next Steps

Learn about activation functions in more detail

Try different optimizers (e.g., Adam, RMSprop)

Implement a small model manually to reinforce forward and backward pass logic
